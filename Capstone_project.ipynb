{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b467f41-56cc-4a15-bc26-424df680017a",
   "metadata": {},
   "source": [
    "# **DEVELOPING A PREDICTIVE MODEL FOR EARLY DETECTION OF MENTAL HEALTH DISORDERS** \n",
    "\n",
    "**DISCLAIMER: Upon use of this model  seek further consultation with a certified healthcare professional in your area**\n",
    "\n",
    "---\n",
    "**Authors:**\n",
    "\n",
    "1. Elvis Wanjohi (Team Leader)\n",
    "\n",
    "2. Jessica Gichimu\n",
    "\n",
    "3. Jesse Ngugi\n",
    "\n",
    "4. Stephen Gachingu\n",
    "\n",
    "5. Latifa Riziki\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Business Understanding**   \n",
    "\n",
    "### 1.1 Business Overview\n",
    "Given the fast-moving pace of economic and technological advancement in today’s world, most people, especially from the younger generation, tend to experience some form of mental health issues in their lifetime. There has been a significant increase in individuals experiencing suicidal ideation. While it may appear that such individuals do not explicitly communicate their distress, a closer examination of their online activity, such as social media posts, comments, and engagement patterns, often reveals underlying emotional states indicative of psychological distress. This could help researchers, students, and practitioners to develop early detection models for mental health support. The goal is to encourage data-driven approaches to mental health awareness, prevention, and support systems. Mental health awareness is primarily in the healthcare and psychology domains, focusing on the assessment, diagnosis, and treatment of mental health conditions. \n",
    "\n",
    "The target audience for this NLP model are health care professionals (such as therapists, psychologists, psychiatrists), and mental health organizations and clinics, where they can prioritize high-risk cases, or monitor trends in mental health conditions across populations. This model could be used to identify early symptoms of the mental health of individuals in our society. We were able to find a brief description of mental health in the Practical Natural Language Processing( A Comprehensive Guide to Building Real-World NLP Systems) book, which gave us the idea of tackling this project. The motivation for the project is try and improve the diagnosis  and treatment of mental health by identifying underlying conditions at an early stage.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2 Problem Statement\n",
    "\n",
    "Mental health care professionals often rely on their experience, degrees and studies to make a diagnosis on the mental health of an individual. We are trying to build a tool that can coexist with their expertise in this field in order to accurately classify individuals with mental health conditions.Mental health care professionals can use statements made by patients  and use our model to analyse the words used and give a diagnosis on the mental health of the patient. This model is meant to work along with the knowledge that our capable health care professionals have, it should be used cautiously to ensure there is minimal or no misdiagnosis. This model will also be available to the public for anyone who would like a quick self assessment should they  protrary signs of a mental breakdown, but they should ensure to do further consultation with their mental health care provider.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 1.3 Business Objective\n",
    "\n",
    "#### 1.3.1 Main Objective \n",
    "\n",
    "The main objective is to build a model that can correctly classify mental health conditions based on given text or statements used by the public or individuals.\n",
    "\n",
    "\n",
    "#### 1.3.2 Specific Objectives\n",
    "\n",
    "The specific objectives of the project are:\n",
    " \n",
    "1. Translate our text data to swahili to make the dataset localized.\n",
    "\n",
    "2. Determine the most common mental health condition.\n",
    "\n",
    "3. Preprocess the data through processes such as; Vectorization and tokenization, handling missing values, and creating new features such as characters, words and sentences.\n",
    "\n",
    "4. Use wordcloud to visualize and identify the most commonly used words for a specific mental health condition.\n",
    "\n",
    "5. Use text length to classify a mental health condition or show correlation  with a mental health condition.\n",
    "\n",
    "6. \n",
    "\n",
    "7. Evaluate the model performance using Precision, Recall, F1score, Accuracy Score and Roc.\n",
    "\n",
    "8. Compare different classification models to determine which performs best for this dataset.\n",
    "\n",
    "9. Scrapping data from an online platform like twitter to show the efficiency of our model.\n",
    "\n",
    "10. Create a translate feature where we can change a english text to swahili text to allow for intrepretability and diversity in our model.\n",
    "\n",
    "#### 1.3.3 Research Questions\n",
    "\n",
    "1. Can we translate our data?\n",
    "\n",
    "2. Which is the most common health condition?\n",
    "\n",
    "3. Which features influenced mental health condition?\n",
    "\n",
    "4. Which words were specific to a certain class?\n",
    "\n",
    "5. Which classifier model had the best Precision, Recall,F1 score, Accuracy score and ROC?\n",
    "\n",
    "6. Which classification model performs best for this dataset?\n",
    "\n",
    "7. Was our model efficient after making use \n",
    "\n",
    "---\n",
    "\n",
    "### 1.5 Success Criteria  \n",
    "\n",
    "The success of this project will be assessed in the following ways:\n",
    "\n",
    "1. It should generate insights into how users feel about their products and services.\n",
    "\n",
    "2. A machine learning model should be successfully developed that automatically determines the sentiment of a tweet based on words and tone used in the text.\n",
    "\n",
    " \n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdd15f7",
   "metadata": {},
   "source": [
    "## DATA PREPROCESSING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3e83f80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import  nltk\n",
    "import re\n",
    "import stopwordsiso as stopwords #provides stopwords for many languages, each identified by its ISO code.\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc235931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>status</th>\n",
       "      <th>text_sw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"My mind is a never-ending cycle of worry, and...</td>\n",
       "      <td>anxiety</td>\n",
       "      <td>\"Akili yangu ni hali isiyobadilika ya wasiwasi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Despite the sun shining and birds singing outs...</td>\n",
       "      <td>bipolar</td>\n",
       "      <td>Licha ya jua kung'aa na ndege wakiimba nje ya ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'm drowning in responsibilities, each one dem...</td>\n",
       "      <td>stress</td>\n",
       "      <td>Mimi ninalemewa na madaraka, kila mmoja akitak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"My emotions shift like the wind, leaving me u...</td>\n",
       "      <td>personality disorder</td>\n",
       "      <td>\"Hisia zangu hubadilika kama upepo, zikiniacha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'm trapped in a whirlwind of thoughts, unable...</td>\n",
       "      <td>anxiety</td>\n",
       "      <td>Nimenaswa na mawazo mengi sana, nashindwa kuka...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text                status  \\\n",
       "0  \"My mind is a never-ending cycle of worry, and...               anxiety   \n",
       "1  Despite the sun shining and birds singing outs...               bipolar   \n",
       "2  I'm drowning in responsibilities, each one dem...                stress   \n",
       "3  \"My emotions shift like the wind, leaving me u...  personality disorder   \n",
       "4  I'm trapped in a whirlwind of thoughts, unable...               anxiety   \n",
       "\n",
       "                                             text_sw  \n",
       "0  \"Akili yangu ni hali isiyobadilika ya wasiwasi...  \n",
       "1  Licha ya jua kung'aa na ndege wakiimba nje ya ...  \n",
       "2  Mimi ninalemewa na madaraka, kila mmoja akitak...  \n",
       "3  \"Hisia zangu hubadilika kama upepo, zikiniacha...  \n",
       "4  Nimenaswa na mawazo mengi sana, nashindwa kuka...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('translated_dataset.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da988f7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status</th>\n",
       "      <th>text_sw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anxiety</td>\n",
       "      <td>\"Akili yangu ni hali isiyobadilika ya wasiwasi, na hata kazi rahisi zaidi nahisi kuwa haiwezi kushindwa.\" \"Ninakuwa na hofu na mashaka, na kila uamuzi unajisikia kama uwanja wa mabomu ya kutegwa chini ya ardhi unaongojea kulipuka.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bipolar</td>\n",
       "      <td>Licha ya jua kung'aa na ndege wakiimba nje ya dirisha langu, huzuni yangu inapungua sana, kana kwamba nimenaswa katika abiso isiyo na mwisho.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stress</td>\n",
       "      <td>Mimi ninalemewa na madaraka, kila mmoja akitaka uangalifu wangu, hata hivyo nahofu kwamba hata nijaribu kadiri gani, huenda nisiweze kamwe kushinda kazi nyingi mno mbele yangu.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>personality disorder</td>\n",
       "      <td>\"Hisia zangu hubadilika kama upepo, zikiniacha nikiwa na wasiwasi kuhusu mimi ni nani kwa kweli. Ninatamani kuwa imara, lakini ninahofia kupoteza uwezo wangu wa kinyonga wa kuchangamana na mazingira yoyote.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>anxiety</td>\n",
       "      <td>Nimenaswa na mawazo mengi sana, nashindwa kukazia fikira kitu chochote huku akili yangu ikiona mandhari mbaya sana, ikiniacha nikiwa hoi na nikiwa nimelemewa na hofu.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 status  \\\n",
       "0               anxiety   \n",
       "1               bipolar   \n",
       "2                stress   \n",
       "3  personality disorder   \n",
       "4               anxiety   \n",
       "\n",
       "                                                                                                                                                                                                                                   text_sw  \n",
       "0  \"Akili yangu ni hali isiyobadilika ya wasiwasi, na hata kazi rahisi zaidi nahisi kuwa haiwezi kushindwa.\" \"Ninakuwa na hofu na mashaka, na kila uamuzi unajisikia kama uwanja wa mabomu ya kutegwa chini ya ardhi unaongojea kulipuka.\"  \n",
       "1                                                                                            Licha ya jua kung'aa na ndege wakiimba nje ya dirisha langu, huzuni yangu inapungua sana, kana kwamba nimenaswa katika abiso isiyo na mwisho.  \n",
       "2                                                         Mimi ninalemewa na madaraka, kila mmoja akitaka uangalifu wangu, hata hivyo nahofu kwamba hata nijaribu kadiri gani, huenda nisiweze kamwe kushinda kazi nyingi mno mbele yangu.  \n",
       "3                          \"Hisia zangu hubadilika kama upepo, zikiniacha nikiwa na wasiwasi kuhusu mimi ni nani kwa kweli. Ninatamani kuwa imara, lakini ninahofia kupoteza uwezo wangu wa kinyonga wa kuchangamana na mazingira yoyote.\"  \n",
       "4                                                                   Nimenaswa na mawazo mengi sana, nashindwa kukazia fikira kitu chochote huku akili yangu ikiona mandhari mbaya sana, ikiniacha nikiwa hoi na nikiwa nimelemewa na hofu.  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth',None)\n",
    "df=data.drop('text', axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab785fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 103488 entries, 0 to 103487\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count   Dtype \n",
      "---  ------   --------------   ----- \n",
      " 0   text     103481 non-null  object\n",
      " 1   status   103488 non-null  object\n",
      " 2   text_sw  103481 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddaa4d41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "status     0\n",
       "text_sw    7\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for missing values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7780cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping rows with missing values\n",
    "df.dropna(axis=0,inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1035f8b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "status     0\n",
       "text_sw    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e3d6fac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3148"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161dfc9c",
   "metadata": {},
   "source": [
    "There are 3,148 duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b9ed0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping duplicated rows\n",
    "df = df.drop_duplicates(keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c08bdd74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status</th>\n",
       "      <th>text_sw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anxiety</td>\n",
       "      <td>\"akili yangu ni hali isiyobadilika ya wasiwasi, na hata kazi rahisi zaidi nahisi kuwa haiwezi kushindwa.\" \"ninakuwa na hofu na mashaka, na kila uamuzi unajisikia kama uwanja wa mabomu ya kutegwa chini ya ardhi unaongojea kulipuka.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bipolar</td>\n",
       "      <td>licha ya jua kung'aa na ndege wakiimba nje ya dirisha langu, huzuni yangu inapungua sana, kana kwamba nimenaswa katika abiso isiyo na mwisho.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stress</td>\n",
       "      <td>mimi ninalemewa na madaraka, kila mmoja akitaka uangalifu wangu, hata hivyo nahofu kwamba hata nijaribu kadiri gani, huenda nisiweze kamwe kushinda kazi nyingi mno mbele yangu.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>personality disorder</td>\n",
       "      <td>\"hisia zangu hubadilika kama upepo, zikiniacha nikiwa na wasiwasi kuhusu mimi ni nani kwa kweli. ninatamani kuwa imara, lakini ninahofia kupoteza uwezo wangu wa kinyonga wa kuchangamana na mazingira yoyote.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>anxiety</td>\n",
       "      <td>nimenaswa na mawazo mengi sana, nashindwa kukazia fikira kitu chochote huku akili yangu ikiona mandhari mbaya sana, ikiniacha nikiwa hoi na nikiwa nimelemewa na hofu.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 status  \\\n",
       "0               anxiety   \n",
       "1               bipolar   \n",
       "2                stress   \n",
       "3  personality disorder   \n",
       "4               anxiety   \n",
       "\n",
       "                                                                                                                                                                                                                                   text_sw  \n",
       "0  \"akili yangu ni hali isiyobadilika ya wasiwasi, na hata kazi rahisi zaidi nahisi kuwa haiwezi kushindwa.\" \"ninakuwa na hofu na mashaka, na kila uamuzi unajisikia kama uwanja wa mabomu ya kutegwa chini ya ardhi unaongojea kulipuka.\"  \n",
       "1                                                                                            licha ya jua kung'aa na ndege wakiimba nje ya dirisha langu, huzuni yangu inapungua sana, kana kwamba nimenaswa katika abiso isiyo na mwisho.  \n",
       "2                                                         mimi ninalemewa na madaraka, kila mmoja akitaka uangalifu wangu, hata hivyo nahofu kwamba hata nijaribu kadiri gani, huenda nisiweze kamwe kushinda kazi nyingi mno mbele yangu.  \n",
       "3                          \"hisia zangu hubadilika kama upepo, zikiniacha nikiwa na wasiwasi kuhusu mimi ni nani kwa kweli. ninatamani kuwa imara, lakini ninahofia kupoteza uwezo wangu wa kinyonga wa kuchangamana na mazingira yoyote.\"  \n",
       "4                                                                   nimenaswa na mawazo mengi sana, nashindwa kukazia fikira kitu chochote huku akili yangu ikiona mandhari mbaya sana, ikiniacha nikiwa hoi na nikiwa nimelemewa na hofu.  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lowercasing\n",
    "df['text_sw']= df['text_sw'].str.lower().str.strip()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bc540f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vitenzi = pd.read_csv('swahili_vitenzi_v1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7df58c9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2185, 146)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vitenzi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "590d54df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['mzizi_wa_neno', 'nafsi_ya_kwanza_umoja_wakati_uliopita',\n",
      "       'nafsi_ya_kwanza_umoja_wakati_uliopo',\n",
      "       'nafsi_ya_kwanza_umoja_wakati_timilifu',\n",
      "       'nafsi_ya_kwanza_umoja_wakati_ujao',\n",
      "       'nafsi_ya_kwanza_wingi_wakati_uliopita',\n",
      "       'nafsi_ya_kwanza_wingi_wakati_uliopo',\n",
      "       'nafsi_ya_kwanza_umoja_wingi_timilifu',\n",
      "       'nafsi_ya_kwanza_wingi_wakati_ujao',\n",
      "       'nafsi_ya_pili_umoja_wakati_uliopita',\n",
      "       ...\n",
      "       'utaye', 'mliye.1', 'mnaye', 'mtaye', 'aliye.1', 'anaye', 'ataye',\n",
      "       'waliye.1', 'wanaye', 'wataye'],\n",
      "      dtype='object', length=146)\n"
     ]
    }
   ],
   "source": [
    "print(vitenzi.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f95ffc6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  mzizi_wa_neno nafsi_ya_kwanza_umoja_wakati_uliopita  \\\n",
      "0          acha                              niliacha   \n",
      "1        achama                            niliachama   \n",
      "2        achana                            niliachana   \n",
      "3     achanisha                         niliachanisha   \n",
      "4         achia                             niliachia   \n",
      "\n",
      "  nafsi_ya_kwanza_umoja_wakati_uliopo nafsi_ya_kwanza_umoja_wakati_timilifu  \\\n",
      "0                            ninaacha                              nimeacha   \n",
      "1                          ninaachama                            nimeachama   \n",
      "2                          ninaachana                            nimeachana   \n",
      "3                       ninaachanisha                         nimeachanisha   \n",
      "4                           ninaachia                             nimeachia   \n",
      "\n",
      "  nafsi_ya_kwanza_umoja_wakati_ujao nafsi_ya_kwanza_wingi_wakati_uliopita  \\\n",
      "0                          nitaacha                              tuliacha   \n",
      "1                        nitaachama                            tuliachama   \n",
      "2                        nitaachana                            tuliachana   \n",
      "3                     nitaachanisha                         tuliachanisha   \n",
      "4                         nitaachia                             tuliachia   \n",
      "\n",
      "  nafsi_ya_kwanza_wingi_wakati_uliopo nafsi_ya_kwanza_umoja_wingi_timilifu  \\\n",
      "0                            tunaacha                             tumeacha   \n",
      "1                          tunaachama                           tumeachama   \n",
      "2                          tunaachana                           tumeachana   \n",
      "3                       tunaachanisha                        tumeachanisha   \n",
      "4                           tunaachia                            tumeachia   \n",
      "\n",
      "  nafsi_ya_kwanza_wingi_wakati_ujao nafsi_ya_pili_umoja_wakati_uliopita  ...  \\\n",
      "0                          tutaacha                             uliacha  ...   \n",
      "1                        tutaachama                           uliachama  ...   \n",
      "2                        tutaachana                           uliachana  ...   \n",
      "3                     tutaachanisha                        uliachanisha  ...   \n",
      "4                         tutaachia                            uliachia  ...   \n",
      "\n",
      "            utaye         mliye.1           mnaye           mtaye  \\\n",
      "0       utayeacha       mliyeacha       mnayeacha       mtayeacha   \n",
      "1     utayeachama     mliyeachama     mnayeachama     mtayeachama   \n",
      "2     utayeachana     mliyeachana     mnayeachana     mtayeachana   \n",
      "3  utayeachanisha  mliyeachanisha  mnayeachanisha  mtayeachanisha   \n",
      "4      utayeachia      mliyeachia      mnayeachia      mtayeachia   \n",
      "\n",
      "          aliye.1           anaye           ataye         waliye.1  \\\n",
      "0       aliyeacha       anayeacha       atayeacha       waliyeacha   \n",
      "1     aliyeachama     anayeachama     atayeachama     waliyeachama   \n",
      "2     aliyeachana     anayeachana     atayeachana     waliyeachana   \n",
      "3  aliyeachanisha  anayeachanisha  atayeachanisha  waliyeachanisha   \n",
      "4      aliyeachia      anayeachia      atayeachia      waliyeachia   \n",
      "\n",
      "            wanaye           wataye  \n",
      "0       wanayeacha       watayeacha  \n",
      "1     wanayeachama     watayeachama  \n",
      "2     wanayeachana     watayeachana  \n",
      "3  wanayeachanisha  watayeachanisha  \n",
      "4      wanayeachia      watayeachia  \n",
      "\n",
      "[5 rows x 146 columns]\n"
     ]
    }
   ],
   "source": [
    "print(vitenzi.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924feb9b",
   "metadata": {},
   "source": [
    "# Convert dataset to long format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94d373f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lookup dictionary: every conjugated form → root\n",
    "verb_dict = {}\n",
    "for _, row in vitenzi.iterrows():\n",
    "    root = row[\"mzizi_wa_neno\"]\n",
    "    for form in row.values:\n",
    "        if isinstance(form, str):\n",
    "            verb_dict[form.lower()] = root.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7c1366c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def swahili_lemmatize_sentence(sentence, verb_dict):\n",
    "    if not isinstance(sentence, str):\n",
    "        return \"\"\n",
    "\n",
    "    # Tokenize sentence\n",
    "    words = re.findall(r\"\\w+|[^\\w\\s]\", sentence.lower())\n",
    "\n",
    "    lemmatized_words = []\n",
    "    for word in words:\n",
    "        lemma = verb_dict.get(word, word)  # replace if found\n",
    "        lemmatized_words.append(lemma)\n",
    "\n",
    "    return \" \".join(lemmatized_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84436dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lemma'] = df['text_sw'].apply(lambda x: swahili_lemmatize_sentence(x, verb_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0dc3e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    \" akili yangu ni hali isiyobadilika ya wasiwasi , na hata kazi rahisi zaidi nahisi wa haiwezi shindwa . \" \" ninakuwa na hofu na mashaka , na kila uamuzi jisikia kama uwanja wa mabomu ya kutegwa chini ya ardhi unaongojea lipuka . \"\n",
       "1                                                                                                   licha ya jua kung ' aa na ndege imba nje ya dirisha langu , huzuni yangu pungua sana , kana kwamba naswa katika abiso isiyo na mwisho .\n",
       "2                                                          mimi ninalemewa na madaraka , kila mmoja akitaka uangalifu wangu , hata hivyo nahofu kwamba hata nijaribu kadiri gani , enda nisiweze kamwe shinda kazi nyingi mno mbele yangu .\n",
       "3                                 \" hisia zangu hubadilika kama upepo , zikiniacha wa na wasiwasi husu mimi ni nani kwa kweli . ninatamani wa imara , lakini ninahofia poteza uwezo wangu wa kinyonga wa changamana na mazingira yoyote . \"\n",
       "4                                                                                  naswa na mawazo mengi sana , nashindwa kazia fikira kitu chochote huku akili yangu ona mandhari mbaya sana , ikiniacha wa hoi na wa nimelemewa na hofu .\n",
       "Name: lemma, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['lemma'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65ee4345",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Common Swahili nouns (expandable list)\n",
    "swahili_nouns = {\n",
    "    \"akili\", \"hali\", \"nguvu\", \"moyo\", \"wasiwasi\", \"hofu\", \"hisia\", \"mawazo\",\n",
    "    \"mashaka\", \"kazi\", \"fikira\", \"mandhari\", \"dirisha\", \"machozi\", \"raha\",\n",
    "    \"upendo\", \"hasira\", \"huruma\", \"mtu\", \"watu\", \"mwana\", \"mwili\", \"siku\",\n",
    "    \"ndoto\", \"maisha\", \"teknolojia\", \"muda\", \"eneo\", \"ubongo\", \"ugonjwa\",\n",
    "    \"ugumu\", \"hali\", \"nchi\", \"moyo\", \"ubaya\", \"ukweli\", \"uzoefu\"\n",
    "}\n",
    "\n",
    "prefix_patterns = [\n",
    "    r'^(?:ni|u|a|tu|m|wa|ki|vi|li|ta|zi|ku|ha|si|isi|mi|wi|na|ya|ka|hu)',\n",
    "]\n",
    "\n",
    "def strip_prefixes(word):\n",
    "    \"\"\"Remove multiple Swahili verb prefixes while avoiding over-stripping.\"\"\"\n",
    "    original = word\n",
    "    for _ in range(3):  # try multiple passes\n",
    "        for pat in prefix_patterns:\n",
    "            new_word = re.sub(pat, '', word)\n",
    "            if len(new_word) < len(word):\n",
    "                word = new_word\n",
    "    return word if word != \"\" else original\n",
    "\n",
    "def clean_complex_forms(word):\n",
    "    \"\"\"Handle negations and suffix patterns.\"\"\"\n",
    "    patterns = [\n",
    "        (r'^isiyo(\\w+)', r'\\1'),\n",
    "        (r'^si(\\w+)', r'\\1'),\n",
    "        (r'^ha(\\w+)', r'\\1'),\n",
    "        (r'(\\w+)wa$', r'\\1'),\n",
    "        (r'(\\w+)ye$', r'\\1'),\n",
    "        (r'(\\w+)ni$', r'\\1'),\n",
    "    ]\n",
    "    for pat, rep in patterns:\n",
    "        new_word = re.sub(pat, rep, word)\n",
    "        if new_word != word:\n",
    "            return new_word\n",
    "    return word\n",
    "\n",
    "def is_probably_noun(word):\n",
    "    \"\"\"Heuristic check to skip lemmatizing nouns.\"\"\"\n",
    "    if word in swahili_nouns:\n",
    "        return True\n",
    "    if len(word) <= 3:\n",
    "        return True\n",
    "    if word.endswith((\"i\", \"a\", \"u\", \"e\")) and not word.endswith((\"ka\", \"la\", \"ta\", \"na\")):\n",
    "        # many nouns end in vowels, but not typical verb suffixes\n",
    "        if not re.match(r'^(ni|ki|zi|wa|ta|na)', word):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def swahili_lemmatize_sentence(sentence, verb_dict):\n",
    "    if not isinstance(sentence, str):\n",
    "        return \"\"\n",
    "\n",
    "    words = re.findall(r\"\\w+|[^\\w\\s]\", sentence.lower())\n",
    "    lemmatized_words = []\n",
    "\n",
    "    for word in words:\n",
    "        # If word looks like a noun, keep it\n",
    "        if is_probably_noun(word):\n",
    "            lemmatized_words.append(word)\n",
    "            continue\n",
    "\n",
    "        # Try direct dictionary lookup\n",
    "        lemma = verb_dict.get(word, None)\n",
    "        if lemma is None:\n",
    "            cleaned = clean_complex_forms(word)\n",
    "            stripped = strip_prefixes(cleaned)\n",
    "            lemma = verb_dict.get(stripped, stripped)\n",
    "\n",
    "        lemmatized_words.append(lemma)\n",
    "\n",
    "    return \" \".join(lemmatized_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1601b6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lemma'] = df['text_sw'].apply(lambda x: swahili_lemmatize_sentence(x, verb_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d1483be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    \" akili yangu ni hali badilika ya wasiwasi , na ta kazi rahisi zaidi hisi kuwa haiwezi kushindwa . \" \" ninaku na hofu na mashaka , na la uamuzi unajisikia kama uwanja wa mabomu ya kutegwa chini ya ardhi unaongojea lipuka . \"\n",
       "1                                                                                                     licha ya jua ng ' aa na ndege imba nje ya dirisha langu , huzuni yangu inapungua sana , kana kwamba naswa tika biso yo na sho .\n",
       "2                                                                             mimi leme na daraka , la mmoja ka uangalifu ngu , ta hivyo hofu kwamba ta jaribu kadiri gani , huenda weze kamwe kushinda kazi nyingi mno mbele yangu .\n",
       "3                                                 \" hisia zangu badilika kama pepo , acha wa na wasiwasi kuhusu mimi ni na kwa kweli . ma kuwa imara , lakini hofia kupoteza wezo ngu wa nyonga wa changamana na mazingira yoyote . \"\n",
       "4                                                                                   naswa na mawazo mengi sana , shind kukazia fikira kitu chochote huku akili yangu ona mandhari mbaya sana , ikiniacha wa hoi na wa eleme na hofu .\n",
       "Name: lemma, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['lemma'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577b1aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def get_word_frequencies(text_series):\n",
    "    all_words = []\n",
    "    for text in text_series.dropna():\n",
    "        words = re.findall(r\"\\b[a-zA-Z’']+\\b\", text.lower())\n",
    "        all_words.extend(words)\n",
    "    return Counter(all_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "606dd87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77802 unique words found.\n"
     ]
    }
   ],
   "source": [
    "word_freqs = get_word_frequencies(df['text_sw'])\n",
    "print(len(word_freqs), \"unique words found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ec9b77c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_detect_nouns(word_freqs, verb_dict, min_freq=2):\n",
    "    noun_prefixes = ('m', 'wa', 'ki', 'vi', 'u', 'ma', 'mi')\n",
    "    verb_prefixes = ('ni', 'na', 'ta', 'me', 'li', 'hu', 'ka', 'si', 'ku', 'ha')\n",
    "    detected_nouns = set()\n",
    "\n",
    "    for word, freq in word_freqs.items():\n",
    "        if freq < min_freq:\n",
    "            continue\n",
    "\n",
    "        # Skip words known to be verbs\n",
    "        if word in verb_dict:\n",
    "            continue\n",
    "\n",
    "        # Likely noun: starts with a noun prefix but not a verb one\n",
    "        if word.startswith(noun_prefixes) and not word.startswith(verb_prefixes):\n",
    "            detected_nouns.add(word)\n",
    "\n",
    "        # Extra rule: words ending in vowels and long enough\n",
    "        elif word.endswith((\"a\", \"i\", \"o\", \"u\", \"e\")) and len(word) > 4:\n",
    "            detected_nouns.add(word)\n",
    "\n",
    "    return detected_nouns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "088fb714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auto-detected nouns: ['wanasaikolojia', 'sitaenda', 'inaporudi', 'wangu', 'vikiniacha', 'whine', 'sitawahi', 'nighewe', 'kuyafanya', 'yapaswavyo', 'ninachojifunza', 'nilimfukuza', 'wanajihisi', 'kushonwa', 'wakaiga', 'amekasirika', 'hajazungumza', 'nisielewe', 'iliyovunjwa', 'bonigani', 'huzama', 'nitawakatisha', 'mipwito', 'nishikwe', 'yanatia', 'pengi', 'sigara', 'kujifahamu', 'yanayoingiliana', 'inakuumiza', 'mungu', 'nisaidieni', 'alivyoanza', 'hawatasikiliza', 'istima', 'unaovuja', 'miongozo', 'nipoa', 'hupigana', 'nijipe']\n"
     ]
    }
   ],
   "source": [
    "auto_nouns = auto_detect_nouns(word_freqs, verb_dict)\n",
    "print(\"Auto-detected nouns:\", list(auto_nouns)[:40])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3cce7abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_noun_list = swahili_nouns.union(auto_nouns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f181e7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_noun_list = swahili_nouns.union(auto_nouns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "63ba5cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 303581 unique Swahili verb forms.\n"
     ]
    }
   ],
   "source": [
    "\\\n",
    "# Flatten all tense columns into one big set of verb forms\n",
    "verb_forms = set(vitenzi.values.flatten())\n",
    "verb_forms = {str(v).lower().strip() for v in verb_forms if isinstance(v, str)}\n",
    "\n",
    "# Also include the roots (mzizi_wa_neno)\n",
    "verb_roots = set(vitenzi['mzizi_wa_neno'].str.lower().tolist())\n",
    "all_verbs = verb_forms.union(verb_roots)\n",
    "\n",
    "print(f\"✅ Loaded {len(all_verbs)} unique Swahili verb forms.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c2e7833d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def refined_auto_detect_nouns(word_freqs, all_verbs, min_freq=2):\n",
    "    noun_prefixes = ('m', 'wa', 'ki', 'vi', 'u', 'ma', 'mi', 'chi', 'ny', 'ji')\n",
    "    verb_prefixes = ('ku', 'ni', 'na', 'ta', 'me', 'li', 'hu', 'ka', 'si', 'ha')\n",
    "    detected_nouns = set()\n",
    "\n",
    "    for word, freq in word_freqs.items():\n",
    "        if freq < min_freq or not word.isalpha():\n",
    "            continue\n",
    "\n",
    "        w = word.lower().strip()\n",
    "\n",
    "        # Skip if the word or root is in verbs list\n",
    "        if w in all_verbs:\n",
    "            continue\n",
    "\n",
    "        # Skip if it starts with a verb prefix and ends with 'a'\n",
    "        if w.startswith(verb_prefixes) and w.endswith(\"a\"):\n",
    "            continue\n",
    "\n",
    "        # Likely noun: starts with a noun prefix, not a verb one\n",
    "        if w.startswith(noun_prefixes) and not w.startswith(verb_prefixes):\n",
    "            detected_nouns.add(w)\n",
    "\n",
    "        # Catch some simple nouns (e.g. \"moyo\", \"akili\", \"shule\")\n",
    "        elif len(w) > 4 and not w.endswith(\"a\") and w not in all_verbs:\n",
    "            detected_nouns.add(w)\n",
    "\n",
    "    return detected_nouns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c96e1a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Auto-detected 12559 likely nouns\n",
      "['prose', 'karne', 'umetatanika', 'plani', 'sinki', 'mapokezi', 'flesheni', 'wanasaikolojia', 'kitekinolojia', 'mwizi', 'hawajali', 'hakiwezi', 'ukajifunza', 'hanifanyi', 'waendelee', 'nyuma', 'inaporudi', 'ulichotaka', 'macy', 'wakapatwa', 'warembo', 'unaloona', 'mono', 'kinaendelea', 'wangu', 'wanapoingia', 'hawaoni', 'vikiniacha', 'whine', 'sitawahi', 'kimedumaa', 'unaofanya', 'itambidi', 'septle', 'ninavyowatumaini', 'nighewe', 'ifuatayo', 'hunidhibiti', 'waonekanapo', 'mwingilio']\n"
     ]
    }
   ],
   "source": [
    "refined_nouns = refined_auto_detect_nouns(word_freqs, all_verbs)\n",
    "print(f\"✅ Auto-detected {len(refined_nouns)} likely nouns\")\n",
    "print(list(refined_nouns)[:40])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "72c0f37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_noun_list = swahili_nouns.union(refined_nouns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0d49ed8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_noun_list = {n for n in final_noun_list if not re.match(r\"^ku[aeiou]\", n)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e7eab91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma for 'walitembea': tembea\n",
      "Lemma for 'unasoma': soma\n",
      "Lemma for 'tutapika': pika\n",
      "Lemma for 'nimefanya': fanya\n",
      "Lemma for 'mwanafunzi': mwanafunzi\n"
     ]
    }
   ],
   "source": [
    "def rule_based_swahili_lemmatizer(word):\n",
    "    # Dictionary of subject prefixes based on person and number\n",
    "    subject_prefixes = {\n",
    "        'ni': 'I', 'u': 'you (sg)', 'a': 'he/she',\n",
    "        'tu': 'we', 'm': 'you (pl)', 'wa': 'they'\n",
    "    }\n",
    "    \n",
    "    # Dictionary of tense markers\n",
    "    tense_markers = {\n",
    "        'na': 'present', 'li': 'past', 'ta': 'future', 'me': 'perfect'\n",
    "    }\n",
    "    \n",
    "    # Very simple morphological parsing\n",
    "    for sp_prefix, sp_name in subject_prefixes.items():\n",
    "        if word.startswith(sp_prefix):\n",
    "            # Remove the subject prefix\n",
    "            stem = word[len(sp_prefix):]\n",
    "            \n",
    "            for t_marker, t_name in tense_markers.items():\n",
    "                if stem.startswith(t_marker):\n",
    "                    # Remove the tense marker\n",
    "                    verb_root = stem[len(t_marker):]\n",
    "                    return verb_root\n",
    "    \n",
    "    # Fallback: simple case for verbs without tense or for other POS\n",
    "    if word.endswith('a'):\n",
    "        return word\n",
    "    \n",
    "    return word\n",
    "\n",
    "# Example usage with verb conjugations\n",
    "print(f\"Lemma for 'walitembea': {rule_based_swahili_lemmatizer('walitembea')}\")\n",
    "print(f\"Lemma for 'unasoma': {rule_based_swahili_lemmatizer('unasoma')}\")\n",
    "print(f\"Lemma for 'tutapika': {rule_based_swahili_lemmatizer('tutapika')}\")\n",
    "print(f\"Lemma for 'nimefanya': {rule_based_swahili_lemmatizer('nimefanya')}\")\n",
    "print(f\"Lemma for 'mwanafunzi': {rule_based_swahili_lemmatizer('mwanafunzi')}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "220fff6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lemma_1']=df['text_sw'].apply(rule_based_swahili_lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3a64856e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    \"akili yangu ni hali isiyobadilika ya wasiwasi, na hata kazi rahisi zaidi nahisi kuwa haiwezi kushindwa.\" \"ninakuwa na hofu na mashaka, na kila uamuzi unajisikia kama uwanja wa mabomu ya kutegwa chini ya ardhi unaongojea kulipuka.\"\n",
       "1                                                                                              licha ya jua kung'aa na ndege wakiimba nje ya dirisha langu, huzuni yangu inapungua sana, kana kwamba nimenaswa katika abiso isiyo na mwisho.\n",
       "2                                                           mimi ninalemewa na madaraka, kila mmoja akitaka uangalifu wangu, hata hivyo nahofu kwamba hata nijaribu kadiri gani, huenda nisiweze kamwe kushinda kazi nyingi mno mbele yangu.\n",
       "3                            \"hisia zangu hubadilika kama upepo, zikiniacha nikiwa na wasiwasi kuhusu mimi ni nani kwa kweli. ninatamani kuwa imara, lakini ninahofia kupoteza uwezo wangu wa kinyonga wa kuchangamana na mazingira yoyote.\"\n",
       "4                                                                         naswa na mawazo mengi sana, nashindwa kukazia fikira kitu chochote huku akili yangu ikiona mandhari mbaya sana, ikiniacha nikiwa hoi na nikiwa nimelemewa na hofu.\n",
       "Name: lemma_1, dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['lemma_1'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "700374f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15ead5e171e547fba5799d4ba7b9b618",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 10.5M/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load the Swahili POS tagger pipeline\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m tagger \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtoken-classification\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmasakhane/swahili-pos-tagger-afroxlmr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maggregation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msimple\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMwanafunzi anasoma kitabu.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m result \u001b[38;5;241m=\u001b[39m tagger(text)\n",
      "File \u001b[1;32mc:\\Users\\Latifa Riziki\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:1027\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1026\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[1;32m-> 1027\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1028\u001b[0m \u001b[43m        \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1029\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1030\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1031\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1032\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1033\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1034\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1035\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1037\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;66;03m# Check which preprocessing classes the pipeline uses\u001b[39;00m\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;66;03m# None values indicate optional classes that the pipeline can run without, we don't raise errors if loading fails\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Latifa Riziki\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py:293\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    287\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    288\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel might be a PyTorch model (ending with `.bin`) but PyTorch is not available. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    289\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to load the model with Tensorflow.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    290\u001b[0m     )\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 293\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    294\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    295\u001b[0m         model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32mc:\\Users\\Latifa Riziki\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:604\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    602\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mconfig_class \u001b[38;5;241m==\u001b[39m config\u001b[38;5;241m.\u001b[39msub_configs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    603\u001b[0m         config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_text_config()\n\u001b[1;32m--> 604\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    605\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    606\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    609\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    610\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Latifa Riziki\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:277\u001b[0m, in \u001b[0;36mrestore_default_dtype.<locals>._wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    279\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[1;32mc:\\Users\\Latifa Riziki\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:4900\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   4890\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   4891\u001b[0m     gguf_file\n\u001b[0;32m   4892\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   4893\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m ((\u001b[38;5;28misinstance\u001b[39m(device_map, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map)\n\u001b[0;32m   4894\u001b[0m ):\n\u001b[0;32m   4895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   4896\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOne or more modules is configured to be mapped to disk. Disk offload is not supported for models \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4897\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloaded from GGUF files.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4898\u001b[0m     )\n\u001b[1;32m-> 4900\u001b[0m checkpoint_files, sharded_metadata \u001b[38;5;241m=\u001b[39m \u001b[43m_get_resolved_checkpoint_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4901\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4902\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4903\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4904\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgguf_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4905\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4906\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4907\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4908\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4909\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4910\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4911\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4912\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4913\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4914\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_auto_class\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   4917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransformers_explicit_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformers_explicit_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4918\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4920\u001b[0m is_sharded \u001b[38;5;241m=\u001b[39m sharded_metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   4921\u001b[0m is_quantized \u001b[38;5;241m=\u001b[39m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Latifa Riziki\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:1037\u001b[0m, in \u001b[0;36m_get_resolved_checkpoint_files\u001b[1;34m(pretrained_model_name_or_path, subfolder, variant, gguf_file, from_tf, from_flax, use_safetensors, cache_dir, force_download, proxies, local_files_only, token, user_agent, revision, commit_hash, is_remote_code, transformers_explicit_filename)\u001b[0m\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1023\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m   1024\u001b[0m     cached_file_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1025\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m: cache_dir,\n\u001b[0;32m   1026\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforce_download\u001b[39m\u001b[38;5;124m\"\u001b[39m: force_download,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1035\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m: commit_hash,\n\u001b[0;32m   1036\u001b[0m     }\n\u001b[1;32m-> 1037\u001b[0m     resolved_archive_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcached_file_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1039\u001b[0m     \u001b[38;5;66;03m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[39;00m\n\u001b[0;32m   1040\u001b[0m     \u001b[38;5;66;03m# result when internet is up, the repo and revision exist, but the file does not.\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_archive_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m filename \u001b[38;5;241m==\u001b[39m _add_variant(SAFE_WEIGHTS_NAME, variant):\n\u001b[0;32m   1042\u001b[0m         \u001b[38;5;66;03m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Latifa Riziki\\anaconda3\\Lib\\site-packages\\transformers\\utils\\hub.py:322\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcached_file\u001b[39m(\n\u001b[0;32m    265\u001b[0m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[0;32m    266\u001b[0m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    268\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    269\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;124;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 322\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    323\u001b[0m     file \u001b[38;5;241m=\u001b[39m file[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "File \u001b[1;32mc:\\Users\\Latifa Riziki\\anaconda3\\Lib\\site-packages\\transformers\\utils\\hub.py:479\u001b[0m, in \u001b[0;36mcached_files\u001b[1;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    478\u001b[0m         \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[1;32m--> 479\u001b[0m         \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    486\u001b[0m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    494\u001b[0m         snapshot_download(\n\u001b[0;32m    495\u001b[0m             path_or_repo_id,\n\u001b[0;32m    496\u001b[0m             allow_patterns\u001b[38;5;241m=\u001b[39mfull_filenames,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    505\u001b[0m             local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    506\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Latifa Riziki\\anaconda3\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Latifa Riziki\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1010\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[0;32m    990\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[0;32m    991\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[0;32m    992\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1007\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m   1008\u001b[0m     )\n\u001b[0;32m   1009\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1010\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[0;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[0;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1018\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[0;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1024\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[0;32m   1025\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1026\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Latifa Riziki\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1171\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[1;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[0;32m   1168\u001b[0m \u001b[38;5;66;03m# Local file doesn't exist or etag isn't a match => retrieve file from remote (or cache)\u001b[39;00m\n\u001b[0;32m   1170\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[1;32m-> 1171\u001b[0m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.incomplete\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1174\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1180\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxet_file_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1183\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(pointer_path):\n\u001b[0;32m   1184\u001b[0m         _create_symlink(blob_path, pointer_path, new_blob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Latifa Riziki\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1738\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[1;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download, etag, xet_file_data)\u001b[0m\n\u001b[0;32m   1731\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m xet_file_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m constants\u001b[38;5;241m.\u001b[39mHF_HUB_DISABLE_XET:\n\u001b[0;32m   1732\u001b[0m             logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m   1733\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXet Storage is enabled for this repo, but the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhf_xet\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m package is not installed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1734\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFalling back to regular HTTP download. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1735\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1736\u001b[0m             )\n\u001b[1;32m-> 1738\u001b[0m         \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1739\u001b[0m \u001b[43m            \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1740\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1741\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1742\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1743\u001b[0m \u001b[43m            \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1745\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1747\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownload complete. Moving file to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1748\u001b[0m _chmod_and_move(incomplete_path, destination_path)\n",
      "File \u001b[1;32mc:\\Users\\Latifa Riziki\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:496\u001b[0m, in \u001b[0;36mhttp_get\u001b[1;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[0;32m    494\u001b[0m new_resume_size \u001b[38;5;241m=\u001b[39m resume_size\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 496\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39mconstants\u001b[38;5;241m.\u001b[39mDOWNLOAD_CHUNK_SIZE):\n\u001b[0;32m    497\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk:  \u001b[38;5;66;03m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[0;32m    498\u001b[0m             progress\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n",
      "File \u001b[1;32mc:\\Users\\Latifa Riziki\\anaconda3\\Lib\\site-packages\\requests\\models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32mc:\\Users\\Latifa Riziki\\anaconda3\\Lib\\site-packages\\urllib3\\response.py:1060\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1059\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1060\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1062\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[0;32m   1063\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\Latifa Riziki\\anaconda3\\Lib\\site-packages\\urllib3\\response.py:949\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    946\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[0;32m    947\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[1;32m--> 949\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    951\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[0;32m    953\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Latifa Riziki\\anaconda3\\Lib\\site-packages\\urllib3\\response.py:873\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[1;34m(self, amt, read1)\u001b[0m\n\u001b[0;32m    870\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    872\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[1;32m--> 873\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    874\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[0;32m    875\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[0;32m    876\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[0;32m    883\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\Latifa Riziki\\anaconda3\\Lib\\site-packages\\urllib3\\response.py:856\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[1;34m(self, amt, read1)\u001b[0m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1()\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Users\\Latifa Riziki\\anaconda3\\Lib\\http\\client.py:479\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[0;32m    478\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[1;32m--> 479\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[0;32m    481\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    482\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32mc:\\Users\\Latifa Riziki\\anaconda3\\Lib\\socket.py:720\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    718\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    719\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 720\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    721\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    722\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Latifa Riziki\\anaconda3\\Lib\\ssl.py:1251\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1248\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1249\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1250\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\Latifa Riziki\\anaconda3\\Lib\\ssl.py:1103\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1101\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1103\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1104\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1105\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the Swahili POS tagger pipeline\n",
    "tagger = pipeline(\"token-classification\", model=\"masakhane/swahili-pos-tagger-afroxlmr\", aggregation_strategy=\"simple\")\n",
    "\n",
    "text = \"Mwanafunzi anasoma kitabu.\"\n",
    "result = tagger(text)\n",
    "\n",
    "print(result)\n",
    "\n",
    "# Expected output will show the word, its tag, and confidence score\n",
    "# Example (simplified):\n",
    "# [\n",
    "#     {'word': 'Mwanafunzi', 'entity_group': 'N', ...},\n",
    "#     {'word': 'anasoma', 'entity_group': 'V', ...},\n",
    "#     {'word': 'kitabu', 'entity_group': 'N', ...}\n",
    "# ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9105c846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lemma for 'walitembea' is: tembea\n",
      "The lemma for 'vitabu' is: kitabu\n",
      "The lemma for 'shule' is: shule\n"
     ]
    }
   ],
   "source": [
    "from simplemma import lemmatize\n",
    "\n",
    "# Lemmatize a conjugated verb\n",
    "word = \"walitembea\"\n",
    "lemma = lemmatize(word, lang='sw')\n",
    "print(f\"The lemma for '{word}' is: {lemma}\")\n",
    "\n",
    "# Lemmatize a plural noun\n",
    "word = \"vitabu\"\n",
    "lemma = lemmatize(word, lang='sw')\n",
    "print(f\"The lemma for '{word}' is: {lemma}\")\n",
    "\n",
    "# Lemmatize a word that is already in its base form\n",
    "word = \"shule\"\n",
    "lemma = lemmatize(word, lang='sw')\n",
    "print(f\"The lemma for '{word}' is: {lemma}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c51648",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simplemma import lemmatize\n",
    "def swahili_lemmatize(word):\n",
    "   return lemmatize(word, lang='sw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "de47a2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lemma_2'] = df['text_sw'].apply(swahili_lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3feb2f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    \"akili yangu ni hali isiyobadilika ya wasiwasi, na hata kazi rahisi zaidi nahisi kuwa haiwezi kushindwa.\" \"ninakuwa na hofu na mashaka, na kila uamuzi unajisikia kama uwanja wa mabomu ya kutegwa chini ya ardhi unaongojea kulipuka.\"\n",
       "1                                                                                              licha ya jua kung'aa na ndege wakiimba nje ya dirisha langu, huzuni yangu inapungua sana, kana kwamba nimenaswa katika abiso isiyo na mwisho.\n",
       "2                                                           mimi ninalemewa na madaraka, kila mmoja akitaka uangalifu wangu, hata hivyo nahofu kwamba hata nijaribu kadiri gani, huenda nisiweze kamwe kushinda kazi nyingi mno mbele yangu.\n",
       "3                            \"hisia zangu hubadilika kama upepo, zikiniacha nikiwa na wasiwasi kuhusu mimi ni nani kwa kweli. ninatamani kuwa imara, lakini ninahofia kupoteza uwezo wangu wa kinyonga wa kuchangamana na mazingira yoyote.\"\n",
       "4                                                                     nimenaswa na mawazo mengi sana, nashindwa kukazia fikira kitu chochote huku akili yangu ikiona mandhari mbaya sana, ikiniacha nikiwa hoi na nikiwa nimelemewa na hofu.\n",
       "Name: lemma_2, dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['lemma_2'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fe94a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "639d5d19706a450e90f943b547f27407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  10%|9         | 220M/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e2d5aa0e12d458c88f939ff1c634ab0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  64%|######3   | 1.43G/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the Masakhane model for Swahili\n",
    "# This model is a multi-task model that includes lemmatization\n",
    "model_name = \"masakhane/swahili-pos-tagger-afroxlmr\"\n",
    "lemmatizer = pipeline(\"token-classification\", model=model_name, aggregation_strategy=\"simple\")\n",
    "\n",
    "text = \"Wanafunzi walitembea shuleni.\"\n",
    "result = lemmatizer(text)\n",
    "\n",
    "# The output will include both the word and its predicted lemma (depending on the pipeline's configuration)\n",
    "# Further processing may be needed to extract only the lemma.\n",
    "for token in result:\n",
    "    print(f\"Word: {token['word']}, POS: {token['entity_group']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
